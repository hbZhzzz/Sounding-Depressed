{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import importlib #\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getenv('MY_CURRENT_PATH'), 'code'))\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from model.load_dataset import modmaDataset\n",
    "from model.run_train_and_eval import run\n",
    "# from model.trainer_no_pt import train_and_test_classify, model_test_cls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "SEED = 2024\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "dataset = modmaDataset()\n",
    "dataset_name = 'modma'\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model.train as mt\n",
    "\n",
    "# importlib.reload(mt)\n",
    "\n",
    "# from model.train import train_or_eval_model\n",
    "\n",
    "LR = 5e-5\n",
    "Weight_Decay = 1e-4\n",
    "EPOCHS = 50\n",
    "BS = 16\n",
    "Dropout = 0.5\n",
    "\n",
    "gamma_1 = 1.0\n",
    "gamma_2 = 0.0 # Deprecated\n",
    "gamma_3 = 0.0 # Deprecated \n",
    "gamma_4 = 1.0\n",
    "gamma_5 = 1.0\n",
    "\n",
    "gamma = [gamma_1, gamma_2, gamma_3, gamma_4, gamma_5]\n",
    "\n",
    "D_audio = 88\n",
    "if D_audio == 1024:\n",
    "    audio_feature_type = 'xlsr' \n",
    "elif D_audio == 88:\n",
    "    audio_feature_type = 'egemaps'\n",
    "elif D_audio == 6073:\n",
    "    audio_feature_type = 'ComPar'\n",
    "elif D_audio == 2041:\n",
    "    audio_feature_type = 'mfcc'\n",
    "\n",
    "classify_flag ='bin' # penta\n",
    "\n",
    "\n",
    "\n",
    "logdir_root = os.path.join(os.getenv('MY_CURRENT_PATH'), 'code/Pers_w/pers_gate/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker Gate Fusion\n",
      "fold: 1, epoch: 1, train_loss: 10.9973, train_acc: 51.22, train_fscore: 50.38, test_loss: 6.6134, test_acc: 36.36, test_fscore: 28.75, time: 3.99 sec\n",
      "fold: 1, epoch: 2, train_loss: 10.2056, train_acc: 68.29, train_fscore: 59.34, test_loss: 6.5703, test_acc: 36.36, test_fscore: 28.75, time: 0.58 sec\n",
      "fold: 1, epoch: 3, train_loss: 10.174, train_acc: 68.29, train_fscore: 59.34, test_loss: 6.2019, test_acc: 45.45, test_fscore: 42.73, time: 1.14 sec\n",
      "fold: 1, epoch: 4, train_loss: 8.9592, train_acc: 70.73, train_fscore: 65.77, test_loss: 6.0636, test_acc: 63.64, test_fscore: 64.85, time: 0.43 sec\n",
      "fold: 1, epoch: 5, train_loss: 9.263, train_acc: 75.61, train_fscore: 72.8, test_loss: 6.0426, test_acc: 72.73, test_fscore: 74.13, time: 0.55 sec\n",
      "fold: 1, epoch: 6, train_loss: 9.707, train_acc: 73.17, train_fscore: 69.4, test_loss: 6.1534, test_acc: 54.55, test_fscore: 54.55, time: 0.51 sec\n",
      "fold: 1, epoch: 7, train_loss: 8.6973, train_acc: 73.17, train_fscore: 69.4, test_loss: 6.1957, test_acc: 54.55, test_fscore: 54.55, time: 0.42 sec\n",
      "fold: 1, epoch: 8, train_loss: 10.2489, train_acc: 73.17, train_fscore: 69.4, test_loss: 6.2003, test_acc: 54.55, test_fscore: 54.55, time: 0.45 sec\n",
      "fold: 1, epoch: 9, train_loss: 9.4725, train_acc: 75.61, train_fscore: 72.8, test_loss: 6.1644, test_acc: 63.64, test_fscore: 64.85, time: 0.48 sec\n",
      "fold: 1, epoch: 10, train_loss: 9.1053, train_acc: 78.05, train_fscore: 76.01, test_loss: 6.1627, test_acc: 63.64, test_fscore: 64.85, time: 0.41 sec\n",
      "fold: 1, epoch: 11, train_loss: 8.5696, train_acc: 78.05, train_fscore: 76.01, test_loss: 6.0992, test_acc: 63.64, test_fscore: 64.85, time: 0.42 sec\n",
      "fold: 1, epoch: 12, train_loss: 8.7846, train_acc: 80.49, train_fscore: 79.06, test_loss: 5.96, test_acc: 72.73, test_fscore: 74.13, time: 0.51 sec\n",
      "fold: 1, epoch: 13, train_loss: 7.9732, train_acc: 85.37, train_fscore: 85.11, test_loss: 5.9014, test_acc: 72.73, test_fscore: 74.13, time: 0.58 sec\n",
      "fold: 1, epoch: 14, train_loss: 8.4446, train_acc: 92.68, train_fscore: 92.63, test_loss: 5.9304, test_acc: 72.73, test_fscore: 74.13, time: 0.51 sec\n",
      "fold: 1, epoch: 15, train_loss: 9.8747, train_acc: 90.24, train_fscore: 89.84, test_loss: 5.9987, test_acc: 72.73, test_fscore: 74.13, time: 0.42 sec\n",
      "fold: 1, epoch: 16, train_loss: 9.978, train_acc: 87.8, train_fscore: 87.12, test_loss: 5.9255, test_acc: 72.73, test_fscore: 74.13, time: 0.37 sec\n",
      "fold: 1, epoch: 17, train_loss: 9.6698, train_acc: 95.12, train_fscore: 95.04, test_loss: 5.7273, test_acc: 72.73, test_fscore: 74.13, time: 0.47 sec\n",
      "fold: 1, epoch: 18, train_loss: 7.8592, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6335, test_acc: 100.0, test_fscore: 100.0, time: 0.57 sec\n",
      "fold: 1, epoch: 19, train_loss: 9.8585, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6275, test_acc: 90.91, test_fscore: 91.26, time: 0.53 sec\n",
      "fold: 1, epoch: 20, train_loss: 10.0051, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6848, test_acc: 90.91, test_fscore: 91.26, time: 0.49 sec\n",
      "fold: 1, epoch: 21, train_loss: 8.9807, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6164, test_acc: 90.91, test_fscore: 91.26, time: 0.44 sec\n",
      "fold: 1, epoch: 22, train_loss: 8.4746, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.5036, test_acc: 100.0, test_fscore: 100.0, time: 0.44 sec\n",
      "fold: 1, epoch: 23, train_loss: 9.6892, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4913, test_acc: 100.0, test_fscore: 100.0, time: 0.46 sec\n",
      "fold: 1, epoch: 24, train_loss: 8.7344, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4944, test_acc: 100.0, test_fscore: 100.0, time: 0.43 sec\n",
      "fold: 1, epoch: 25, train_loss: 11.4717, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.5146, test_acc: 100.0, test_fscore: 100.0, time: 0.41 sec\n",
      "fold: 1, epoch: 26, train_loss: 9.6389, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.5187, test_acc: 100.0, test_fscore: 100.0, time: 0.46 sec\n",
      "fold: 1, epoch: 27, train_loss: 8.3837, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4968, test_acc: 100.0, test_fscore: 100.0, time: 0.44 sec\n",
      "fold: 1, epoch: 28, train_loss: 9.5309, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.5028, test_acc: 100.0, test_fscore: 100.0, time: 0.39 sec\n",
      "fold: 1, epoch: 29, train_loss: 8.383, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.5063, test_acc: 100.0, test_fscore: 100.0, time: 0.54 sec\n",
      "fold: 1, epoch: 30, train_loss: 7.7912, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4966, test_acc: 100.0, test_fscore: 100.0, time: 0.55 sec\n",
      "fold: 1, epoch: 31, train_loss: 8.7133, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4783, test_acc: 100.0, test_fscore: 100.0, time: 0.57 sec\n",
      "fold: 1, epoch: 32, train_loss: 9.5176, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4769, test_acc: 100.0, test_fscore: 100.0, time: 0.5 sec\n",
      "fold: 1, epoch: 33, train_loss: 9.8223, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4819, test_acc: 100.0, test_fscore: 100.0, time: 0.49 sec\n",
      "fold: 1, epoch: 34, train_loss: 8.6318, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4985, test_acc: 100.0, test_fscore: 100.0, time: 0.48 sec\n",
      "fold: 1, epoch: 35, train_loss: 9.3554, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.5021, test_acc: 100.0, test_fscore: 100.0, time: 0.47 sec\n",
      "fold: 1, epoch: 36, train_loss: 9.7086, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4891, test_acc: 100.0, test_fscore: 100.0, time: 0.54 sec\n",
      "fold: 1, epoch: 37, train_loss: 9.1946, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4794, test_acc: 100.0, test_fscore: 100.0, time: 0.45 sec\n",
      "fold: 1, epoch: 38, train_loss: 9.6742, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4783, test_acc: 100.0, test_fscore: 100.0, time: 0.52 sec\n",
      "fold: 1, epoch: 39, train_loss: 7.9931, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4738, test_acc: 100.0, test_fscore: 100.0, time: 0.43 sec\n",
      "fold: 1, epoch: 40, train_loss: 9.2649, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4682, test_acc: 100.0, test_fscore: 100.0, time: 0.38 sec\n",
      "fold: 1, epoch: 41, train_loss: 7.3716, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.476, test_acc: 100.0, test_fscore: 100.0, time: 0.38 sec\n",
      "fold: 1, epoch: 42, train_loss: 7.5105, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4867, test_acc: 100.0, test_fscore: 100.0, time: 0.37 sec\n",
      "fold: 1, epoch: 43, train_loss: 7.8835, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4729, test_acc: 100.0, test_fscore: 100.0, time: 0.4 sec\n",
      "fold: 1, epoch: 44, train_loss: 7.6727, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4606, test_acc: 100.0, test_fscore: 100.0, time: 0.39 sec\n",
      "fold: 1, epoch: 45, train_loss: 8.2142, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4663, test_acc: 100.0, test_fscore: 100.0, time: 0.43 sec\n",
      "fold: 1, epoch: 46, train_loss: 8.6288, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4783, test_acc: 100.0, test_fscore: 100.0, time: 0.49 sec\n",
      "fold: 1, epoch: 47, train_loss: 8.7272, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4768, test_acc: 100.0, test_fscore: 100.0, time: 0.4 sec\n",
      "fold: 1, epoch: 48, train_loss: 9.8249, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4743, test_acc: 100.0, test_fscore: 100.0, time: 0.52 sec\n",
      "fold: 1, epoch: 49, train_loss: 8.7173, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4652, test_acc: 100.0, test_fscore: 100.0, time: 0.49 sec\n",
      "fold: 1, epoch: 50, train_loss: 10.4451, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4639, test_acc: 100.0, test_fscore: 100.0, time: 0.44 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000         3\n",
      "           1     1.0000    1.0000    1.0000         8\n",
      "\n",
      "    accuracy                         1.0000        11\n",
      "   macro avg     1.0000    1.0000    1.0000        11\n",
      "weighted avg     1.0000    1.0000    1.0000        11\n",
      "\n",
      "[[3 0]\n",
      " [0 8]]\n",
      "best model saved in:  /home/zhanghb/program/MyPaperProgram/fairness_personality/code/fusion_spk/speaker_gate_fusion/runs/11-18_19-26_modma_bin_lr5e-05_dropout0.5_bs16_epochs50_fold1/saved_model/modma_model_fold1_fs100.0000.pt\n",
      "Test performance..\n",
      "fold: 1, F-Score: 100.0\n",
      "fold: 1, F-Score-index: 18\n",
      "fold: 1, Acc: 100.0\n",
      "fold: 1, Acc-index: 18\n",
      "Speaker Gate Fusion\n",
      "fold: 2, epoch: 1, train_loss: 10.2434, train_acc: 58.54, train_fscore: 56.7, test_loss: 6.1529, test_acc: 54.55, test_fscore: 44.92, time: 0.57 sec\n",
      "fold: 2, epoch: 2, train_loss: 10.0271, train_acc: 53.66, train_fscore: 48.85, test_loss: 6.0321, test_acc: 63.64, test_fscore: 59.85, time: 0.55 sec\n",
      "fold: 2, epoch: 3, train_loss: 7.9455, train_acc: 73.17, train_fscore: 72.88, test_loss: 5.9639, test_acc: 63.64, test_fscore: 59.85, time: 0.6 sec\n",
      "fold: 2, epoch: 4, train_loss: 9.3558, train_acc: 80.49, train_fscore: 80.49, test_loss: 5.9438, test_acc: 63.64, test_fscore: 59.85, time: 0.45 sec\n",
      "fold: 2, epoch: 5, train_loss: 9.5152, train_acc: 78.05, train_fscore: 77.08, test_loss: 5.9518, test_acc: 63.64, test_fscore: 59.85, time: 0.4 sec\n",
      "fold: 2, epoch: 6, train_loss: 9.8961, train_acc: 80.49, train_fscore: 79.83, test_loss: 5.9577, test_acc: 63.64, test_fscore: 59.85, time: 0.42 sec\n",
      "fold: 2, epoch: 7, train_loss: 9.5143, train_acc: 85.37, train_fscore: 84.87, test_loss: 5.9608, test_acc: 63.64, test_fscore: 59.85, time: 0.51 sec\n",
      "fold: 2, epoch: 8, train_loss: 8.6269, train_acc: 90.24, train_fscore: 90.18, test_loss: 5.9652, test_acc: 63.64, test_fscore: 59.85, time: 0.57 sec\n",
      "fold: 2, epoch: 9, train_loss: 9.1949, train_acc: 90.24, train_fscore: 90.18, test_loss: 5.9702, test_acc: 63.64, test_fscore: 59.85, time: 0.71 sec\n",
      "fold: 2, epoch: 10, train_loss: 9.3513, train_acc: 87.8, train_fscore: 87.5, test_loss: 5.9705, test_acc: 63.64, test_fscore: 59.85, time: 0.36 sec\n",
      "fold: 2, epoch: 11, train_loss: 8.9239, train_acc: 92.68, train_fscore: 92.67, test_loss: 5.9528, test_acc: 81.82, test_fscore: 81.82, time: 0.45 sec\n",
      "fold: 2, epoch: 12, train_loss: 9.4219, train_acc: 95.12, train_fscore: 95.09, test_loss: 5.9384, test_acc: 81.82, test_fscore: 81.82, time: 0.42 sec\n",
      "fold: 2, epoch: 13, train_loss: 8.2411, train_acc: 97.56, train_fscore: 97.56, test_loss: 5.9453, test_acc: 81.82, test_fscore: 81.82, time: 0.51 sec\n",
      "fold: 2, epoch: 14, train_loss: 8.6435, train_acc: 97.56, train_fscore: 97.56, test_loss: 5.9606, test_acc: 81.82, test_fscore: 81.82, time: 0.45 sec\n",
      "fold: 2, epoch: 15, train_loss: 8.4295, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.0024, test_acc: 81.82, test_fscore: 81.82, time: 0.59 sec\n",
      "fold: 2, epoch: 16, train_loss: 8.7152, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.0611, test_acc: 81.82, test_fscore: 81.82, time: 0.6 sec\n",
      "fold: 2, epoch: 17, train_loss: 10.0032, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.1293, test_acc: 81.82, test_fscore: 81.82, time: 0.46 sec\n",
      "fold: 2, epoch: 18, train_loss: 11.0328, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.1966, test_acc: 81.82, test_fscore: 81.82, time: 0.41 sec\n",
      "fold: 2, epoch: 19, train_loss: 8.8027, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2793, test_acc: 81.82, test_fscore: 81.82, time: 0.44 sec\n",
      "fold: 2, epoch: 20, train_loss: 8.8367, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.3736, test_acc: 81.82, test_fscore: 81.82, time: 0.44 sec\n",
      "fold: 2, epoch: 21, train_loss: 9.363, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.4646, test_acc: 81.82, test_fscore: 81.82, time: 0.58 sec\n",
      "fold: 2, epoch: 22, train_loss: 8.6163, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.532, test_acc: 81.82, test_fscore: 81.82, time: 0.38 sec\n",
      "fold: 2, epoch: 23, train_loss: 10.2407, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.5331, test_acc: 81.82, test_fscore: 81.82, time: 0.49 sec\n",
      "fold: 2, epoch: 24, train_loss: 8.7386, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.508, test_acc: 81.82, test_fscore: 81.82, time: 0.58 sec\n",
      "fold: 2, epoch: 25, train_loss: 8.8235, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.5009, test_acc: 81.82, test_fscore: 81.82, time: 0.6 sec\n",
      "fold: 2, epoch: 26, train_loss: 9.1879, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.4825, test_acc: 81.82, test_fscore: 81.82, time: 0.56 sec\n",
      "fold: 2, epoch: 27, train_loss: 8.3077, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.4235, test_acc: 81.82, test_fscore: 81.82, time: 0.4 sec\n",
      "fold: 2, epoch: 28, train_loss: 10.108, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.3626, test_acc: 81.82, test_fscore: 81.82, time: 0.5 sec\n",
      "fold: 2, epoch: 29, train_loss: 10.7716, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.3435, test_acc: 81.82, test_fscore: 81.82, time: 0.54 sec\n",
      "fold: 2, epoch: 30, train_loss: 8.5986, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.3184, test_acc: 81.82, test_fscore: 81.82, time: 0.49 sec\n",
      "fold: 2, epoch: 31, train_loss: 8.0383, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2865, test_acc: 81.82, test_fscore: 81.82, time: 0.66 sec\n",
      "fold: 2, epoch: 32, train_loss: 10.6984, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.268, test_acc: 81.82, test_fscore: 81.82, time: 0.56 sec\n",
      "fold: 2, epoch: 33, train_loss: 8.0309, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2335, test_acc: 81.82, test_fscore: 81.82, time: 0.58 sec\n",
      "fold: 2, epoch: 34, train_loss: 8.2785, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2314, test_acc: 81.82, test_fscore: 81.82, time: 0.58 sec\n",
      "fold: 2, epoch: 35, train_loss: 9.0901, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2397, test_acc: 81.82, test_fscore: 81.82, time: 0.51 sec\n",
      "fold: 2, epoch: 36, train_loss: 7.6588, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2745, test_acc: 81.82, test_fscore: 81.82, time: 0.43 sec\n",
      "fold: 2, epoch: 37, train_loss: 7.3922, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2418, test_acc: 81.82, test_fscore: 81.82, time: 0.45 sec\n",
      "fold: 2, epoch: 38, train_loss: 7.9807, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2193, test_acc: 90.91, test_fscore: 91.06, time: 0.49 sec\n",
      "fold: 2, epoch: 39, train_loss: 8.0189, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2125, test_acc: 90.91, test_fscore: 91.06, time: 0.4 sec\n",
      "fold: 2, epoch: 40, train_loss: 10.1597, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2072, test_acc: 90.91, test_fscore: 91.06, time: 0.53 sec\n",
      "fold: 2, epoch: 41, train_loss: 6.9661, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.201, test_acc: 90.91, test_fscore: 91.06, time: 0.5 sec\n",
      "fold: 2, epoch: 42, train_loss: 7.3934, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.1943, test_acc: 90.91, test_fscore: 91.06, time: 0.43 sec\n",
      "fold: 2, epoch: 43, train_loss: 8.5877, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.1892, test_acc: 90.91, test_fscore: 91.06, time: 0.5 sec\n",
      "fold: 2, epoch: 44, train_loss: 9.0206, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2025, test_acc: 90.91, test_fscore: 91.06, time: 0.53 sec\n",
      "fold: 2, epoch: 45, train_loss: 7.3865, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.205, test_acc: 90.91, test_fscore: 91.06, time: 0.44 sec\n",
      "fold: 2, epoch: 46, train_loss: 9.418, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2024, test_acc: 90.91, test_fscore: 91.06, time: 0.53 sec\n",
      "fold: 2, epoch: 47, train_loss: 8.2459, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2113, test_acc: 90.91, test_fscore: 91.06, time: 0.51 sec\n",
      "fold: 2, epoch: 48, train_loss: 10.0462, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2148, test_acc: 90.91, test_fscore: 91.06, time: 0.47 sec\n",
      "fold: 2, epoch: 49, train_loss: 8.0005, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2207, test_acc: 90.91, test_fscore: 91.06, time: 0.44 sec\n",
      "fold: 2, epoch: 50, train_loss: 9.6109, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2317, test_acc: 90.91, test_fscore: 91.06, time: 0.61 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.8571    0.9231         7\n",
      "           1     0.8000    1.0000    0.8889         4\n",
      "\n",
      "    accuracy                         0.9091        11\n",
      "   macro avg     0.9000    0.9286    0.9060        11\n",
      "weighted avg     0.9273    0.9091    0.9106        11\n",
      "\n",
      "[[6 1]\n",
      " [0 4]]\n",
      "best model saved in:  /home/zhanghb/program/MyPaperProgram/fairness_personality/code/fusion_spk/speaker_gate_fusion/runs/11-18_19-26_modma_bin_lr5e-05_dropout0.5_bs16_epochs50_fold2/saved_model/modma_model_fold2_fs91.0600.pt\n",
      "Test performance..\n",
      "fold: 2, F-Score: 91.06\n",
      "fold: 2, F-Score-index: 38\n",
      "fold: 2, Acc: 90.91\n",
      "fold: 2, Acc-index: 38\n",
      "Speaker Gate Fusion\n",
      "fold: 3, epoch: 1, train_loss: 8.5634, train_acc: 50.0, train_fscore: 49.39, test_loss: 5.9622, test_acc: 80.0, test_fscore: 78.1, time: 0.72 sec\n",
      "fold: 3, epoch: 2, train_loss: 8.9288, train_acc: 66.67, train_fscore: 62.42, test_loss: 5.866, test_acc: 90.0, test_fscore: 89.67, time: 0.46 sec\n",
      "fold: 3, epoch: 3, train_loss: 8.1437, train_acc: 78.57, train_fscore: 78.0, test_loss: 5.7829, test_acc: 90.0, test_fscore: 89.67, time: 0.74 sec\n",
      "fold: 3, epoch: 4, train_loss: 9.1691, train_acc: 78.57, train_fscore: 78.0, test_loss: 5.7464, test_acc: 80.0, test_fscore: 78.1, time: 0.68 sec\n",
      "fold: 3, epoch: 5, train_loss: 9.3216, train_acc: 80.95, train_fscore: 80.59, test_loss: 5.7485, test_acc: 100.0, test_fscore: 100.0, time: 0.59 sec\n",
      "fold: 3, epoch: 6, train_loss: 8.7091, train_acc: 85.71, train_fscore: 85.62, test_loss: 5.758, test_acc: 90.0, test_fscore: 90.1, time: 0.44 sec\n",
      "fold: 3, epoch: 7, train_loss: 8.4621, train_acc: 88.1, train_fscore: 88.06, test_loss: 5.7039, test_acc: 90.0, test_fscore: 89.67, time: 0.41 sec\n",
      "fold: 3, epoch: 8, train_loss: 10.0454, train_acc: 80.95, train_fscore: 80.59, test_loss: 5.6758, test_acc: 90.0, test_fscore: 89.67, time: 0.56 sec\n",
      "fold: 3, epoch: 9, train_loss: 8.0497, train_acc: 83.33, train_fscore: 82.89, test_loss: 5.6644, test_acc: 90.0, test_fscore: 89.67, time: 0.49 sec\n",
      "fold: 3, epoch: 10, train_loss: 8.5276, train_acc: 88.1, train_fscore: 88.06, test_loss: 5.6922, test_acc: 90.0, test_fscore: 90.1, time: 0.52 sec\n",
      "fold: 3, epoch: 11, train_loss: 8.5122, train_acc: 92.86, train_fscore: 92.87, test_loss: 5.7048, test_acc: 80.0, test_fscore: 80.0, time: 0.56 sec\n",
      "fold: 3, epoch: 12, train_loss: 9.1526, train_acc: 95.24, train_fscore: 95.24, test_loss: 5.6626, test_acc: 90.0, test_fscore: 90.1, time: 0.57 sec\n",
      "fold: 3, epoch: 13, train_loss: 8.3772, train_acc: 95.24, train_fscore: 95.24, test_loss: 5.6339, test_acc: 90.0, test_fscore: 90.1, time: 0.8 sec\n",
      "fold: 3, epoch: 14, train_loss: 7.7508, train_acc: 95.24, train_fscore: 95.24, test_loss: 5.6231, test_acc: 90.0, test_fscore: 90.1, time: 0.59 sec\n",
      "fold: 3, epoch: 15, train_loss: 8.3178, train_acc: 95.24, train_fscore: 95.24, test_loss: 5.6462, test_acc: 90.0, test_fscore: 90.1, time: 0.53 sec\n",
      "fold: 3, epoch: 16, train_loss: 7.3801, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.665, test_acc: 90.0, test_fscore: 90.1, time: 0.43 sec\n",
      "fold: 3, epoch: 17, train_loss: 6.877, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6431, test_acc: 90.0, test_fscore: 90.1, time: 0.55 sec\n",
      "fold: 3, epoch: 18, train_loss: 7.5456, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6466, test_acc: 80.0, test_fscore: 80.0, time: 0.74 sec\n",
      "fold: 3, epoch: 19, train_loss: 10.5599, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6614, test_acc: 80.0, test_fscore: 80.0, time: 0.7 sec\n",
      "fold: 3, epoch: 20, train_loss: 9.394, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7392, test_acc: 80.0, test_fscore: 80.0, time: 0.57 sec\n",
      "fold: 3, epoch: 21, train_loss: 8.6013, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7724, test_acc: 80.0, test_fscore: 80.0, time: 0.56 sec\n",
      "fold: 3, epoch: 22, train_loss: 8.9117, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8053, test_acc: 80.0, test_fscore: 80.0, time: 0.51 sec\n",
      "fold: 3, epoch: 23, train_loss: 9.3313, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7768, test_acc: 80.0, test_fscore: 80.0, time: 0.52 sec\n",
      "fold: 3, epoch: 24, train_loss: 9.2245, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7504, test_acc: 80.0, test_fscore: 80.0, time: 0.4 sec\n",
      "fold: 3, epoch: 25, train_loss: 8.6971, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7233, test_acc: 80.0, test_fscore: 80.0, time: 0.47 sec\n",
      "fold: 3, epoch: 26, train_loss: 9.0711, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7502, test_acc: 80.0, test_fscore: 80.0, time: 0.47 sec\n",
      "fold: 3, epoch: 27, train_loss: 7.9728, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8116, test_acc: 90.0, test_fscore: 90.1, time: 0.41 sec\n",
      "fold: 3, epoch: 28, train_loss: 8.2915, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8898, test_acc: 80.0, test_fscore: 80.0, time: 0.48 sec\n",
      "fold: 3, epoch: 29, train_loss: 9.3397, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8204, test_acc: 90.0, test_fscore: 90.1, time: 0.49 sec\n",
      "fold: 3, epoch: 30, train_loss: 8.6633, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6798, test_acc: 80.0, test_fscore: 80.0, time: 0.48 sec\n",
      "fold: 3, epoch: 31, train_loss: 8.9176, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6565, test_acc: 80.0, test_fscore: 80.0, time: 0.67 sec\n",
      "fold: 3, epoch: 32, train_loss: 9.8207, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6908, test_acc: 90.0, test_fscore: 90.1, time: 0.56 sec\n",
      "fold: 3, epoch: 33, train_loss: 9.5912, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7103, test_acc: 90.0, test_fscore: 90.1, time: 0.57 sec\n",
      "fold: 3, epoch: 34, train_loss: 8.423, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7487, test_acc: 90.0, test_fscore: 90.1, time: 0.5 sec\n",
      "fold: 3, epoch: 35, train_loss: 8.1909, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7318, test_acc: 90.0, test_fscore: 90.1, time: 0.41 sec\n",
      "fold: 3, epoch: 36, train_loss: 8.1516, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6638, test_acc: 90.0, test_fscore: 90.1, time: 0.39 sec\n",
      "fold: 3, epoch: 37, train_loss: 10.282, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6195, test_acc: 90.0, test_fscore: 90.1, time: 0.65 sec\n",
      "fold: 3, epoch: 38, train_loss: 8.9906, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.652, test_acc: 90.0, test_fscore: 90.1, time: 0.74 sec\n",
      "fold: 3, epoch: 39, train_loss: 7.9537, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8185, test_acc: 80.0, test_fscore: 80.0, time: 0.72 sec\n",
      "fold: 3, epoch: 40, train_loss: 7.6686, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.9963, test_acc: 80.0, test_fscore: 80.0, time: 0.63 sec\n",
      "fold: 3, epoch: 41, train_loss: 8.8759, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8378, test_acc: 80.0, test_fscore: 80.0, time: 0.6 sec\n",
      "fold: 3, epoch: 42, train_loss: 8.0041, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6422, test_acc: 90.0, test_fscore: 90.1, time: 0.51 sec\n",
      "fold: 3, epoch: 43, train_loss: 8.686, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6257, test_acc: 90.0, test_fscore: 90.1, time: 0.48 sec\n",
      "fold: 3, epoch: 44, train_loss: 8.1831, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7506, test_acc: 80.0, test_fscore: 80.0, time: 0.52 sec\n",
      "fold: 3, epoch: 45, train_loss: 8.512, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8214, test_acc: 80.0, test_fscore: 80.0, time: 0.48 sec\n",
      "fold: 3, epoch: 46, train_loss: 8.5296, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8301, test_acc: 80.0, test_fscore: 80.0, time: 0.64 sec\n",
      "fold: 3, epoch: 47, train_loss: 9.2416, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8035, test_acc: 80.0, test_fscore: 80.0, time: 0.55 sec\n",
      "fold: 3, epoch: 48, train_loss: 8.0678, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.744, test_acc: 80.0, test_fscore: 80.0, time: 0.64 sec\n",
      "fold: 3, epoch: 49, train_loss: 8.5623, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7984, test_acc: 80.0, test_fscore: 80.0, time: 0.5 sec\n",
      "fold: 3, epoch: 50, train_loss: 8.8798, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7764, test_acc: 80.0, test_fscore: 80.0, time: 0.41 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000         6\n",
      "           1     1.0000    1.0000    1.0000         4\n",
      "\n",
      "    accuracy                         1.0000        10\n",
      "   macro avg     1.0000    1.0000    1.0000        10\n",
      "weighted avg     1.0000    1.0000    1.0000        10\n",
      "\n",
      "[[6 0]\n",
      " [0 4]]\n",
      "best model saved in:  /home/zhanghb/program/MyPaperProgram/fairness_personality/code/fusion_spk/speaker_gate_fusion/runs/11-18_19-27_modma_bin_lr5e-05_dropout0.5_bs16_epochs50_fold3/saved_model/modma_model_fold3_fs100.0000.pt\n",
      "Test performance..\n",
      "fold: 3, F-Score: 100.0\n",
      "fold: 3, F-Score-index: 5\n",
      "fold: 3, Acc: 100.0\n",
      "fold: 3, Acc-index: 5\n",
      "Speaker Gate Fusion\n",
      "fold: 4, epoch: 1, train_loss: 10.4932, train_acc: 54.76, train_fscore: 51.43, test_loss: 6.0923, test_acc: 60.0, test_fscore: 60.0, time: 0.63 sec\n",
      "fold: 4, epoch: 2, train_loss: 8.1975, train_acc: 59.52, train_fscore: 59.5, test_loss: 6.2112, test_acc: 40.0, test_fscore: 40.0, time: 0.43 sec\n",
      "fold: 4, epoch: 3, train_loss: 9.9248, train_acc: 54.76, train_fscore: 48.15, test_loss: 6.1079, test_acc: 50.0, test_fscore: 52.53, time: 0.54 sec\n",
      "fold: 4, epoch: 4, train_loss: 9.6751, train_acc: 83.33, train_fscore: 83.32, test_loss: 5.9678, test_acc: 70.0, test_fscore: 72.97, time: 0.5 sec\n",
      "fold: 4, epoch: 5, train_loss: 8.2407, train_acc: 73.81, train_fscore: 73.43, test_loss: 5.8509, test_acc: 80.0, test_fscore: 80.0, time: 0.47 sec\n",
      "fold: 4, epoch: 6, train_loss: 10.8886, train_acc: 73.81, train_fscore: 72.55, test_loss: 5.825, test_acc: 80.0, test_fscore: 80.0, time: 0.47 sec\n",
      "fold: 4, epoch: 7, train_loss: 9.6974, train_acc: 73.81, train_fscore: 72.55, test_loss: 5.8401, test_acc: 80.0, test_fscore: 80.0, time: 0.41 sec\n",
      "fold: 4, epoch: 8, train_loss: 7.9708, train_acc: 80.95, train_fscore: 80.78, test_loss: 6.0, test_acc: 80.0, test_fscore: 81.9, time: 0.53 sec\n",
      "fold: 4, epoch: 9, train_loss: 9.7416, train_acc: 95.24, train_fscore: 95.23, test_loss: 6.0319, test_acc: 80.0, test_fscore: 81.9, time: 0.57 sec\n",
      "fold: 4, epoch: 10, train_loss: 8.7315, train_acc: 92.86, train_fscore: 92.85, test_loss: 5.9695, test_acc: 80.0, test_fscore: 81.9, time: 0.42 sec\n",
      "fold: 4, epoch: 11, train_loss: 8.8025, train_acc: 92.86, train_fscore: 92.85, test_loss: 5.8616, test_acc: 80.0, test_fscore: 81.9, time: 0.5 sec\n",
      "fold: 4, epoch: 12, train_loss: 9.8517, train_acc: 90.48, train_fscore: 90.48, test_loss: 5.8465, test_acc: 80.0, test_fscore: 81.9, time: 0.37 sec\n",
      "fold: 4, epoch: 13, train_loss: 8.8272, train_acc: 92.86, train_fscore: 92.85, test_loss: 5.9242, test_acc: 80.0, test_fscore: 81.9, time: 0.48 sec\n",
      "fold: 4, epoch: 14, train_loss: 9.0617, train_acc: 92.86, train_fscore: 92.85, test_loss: 5.9156, test_acc: 80.0, test_fscore: 81.9, time: 0.57 sec\n",
      "fold: 4, epoch: 15, train_loss: 9.473, train_acc: 97.62, train_fscore: 97.62, test_loss: 5.7915, test_acc: 90.0, test_fscore: 90.67, time: 0.68 sec\n",
      "fold: 4, epoch: 16, train_loss: 8.6466, train_acc: 97.62, train_fscore: 97.62, test_loss: 5.7368, test_acc: 90.0, test_fscore: 90.67, time: 0.54 sec\n",
      "fold: 4, epoch: 17, train_loss: 10.0714, train_acc: 97.62, train_fscore: 97.62, test_loss: 5.8357, test_acc: 80.0, test_fscore: 81.9, time: 0.44 sec\n",
      "fold: 4, epoch: 18, train_loss: 9.4275, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8379, test_acc: 80.0, test_fscore: 81.9, time: 0.48 sec\n",
      "fold: 4, epoch: 19, train_loss: 9.0231, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6629, test_acc: 90.0, test_fscore: 90.67, time: 0.59 sec\n",
      "fold: 4, epoch: 20, train_loss: 6.9662, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.5587, test_acc: 90.0, test_fscore: 90.67, time: 0.68 sec\n",
      "fold: 4, epoch: 21, train_loss: 8.762, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.5722, test_acc: 90.0, test_fscore: 90.67, time: 0.76 sec\n",
      "fold: 4, epoch: 22, train_loss: 6.893, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6805, test_acc: 80.0, test_fscore: 81.9, time: 0.41 sec\n",
      "fold: 4, epoch: 23, train_loss: 8.9102, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7491, test_acc: 80.0, test_fscore: 81.9, time: 0.45 sec\n",
      "fold: 4, epoch: 24, train_loss: 7.5204, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6622, test_acc: 80.0, test_fscore: 81.9, time: 0.59 sec\n",
      "fold: 4, epoch: 25, train_loss: 8.0407, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.5373, test_acc: 90.0, test_fscore: 90.67, time: 0.68 sec\n",
      "fold: 4, epoch: 26, train_loss: 9.525, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4994, test_acc: 100.0, test_fscore: 100.0, time: 0.49 sec\n",
      "fold: 4, epoch: 27, train_loss: 9.4231, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4938, test_acc: 100.0, test_fscore: 100.0, time: 0.48 sec\n",
      "fold: 4, epoch: 28, train_loss: 8.0063, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.512, test_acc: 100.0, test_fscore: 100.0, time: 0.44 sec\n",
      "fold: 4, epoch: 29, train_loss: 7.8005, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.5248, test_acc: 90.0, test_fscore: 90.67, time: 0.46 sec\n",
      "fold: 4, epoch: 30, train_loss: 8.3405, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.5203, test_acc: 90.0, test_fscore: 90.67, time: 0.45 sec\n",
      "fold: 4, epoch: 31, train_loss: 7.6922, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4999, test_acc: 90.0, test_fscore: 90.67, time: 0.63 sec\n",
      "fold: 4, epoch: 32, train_loss: 8.7921, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4799, test_acc: 90.0, test_fscore: 90.67, time: 0.45 sec\n",
      "fold: 4, epoch: 33, train_loss: 8.2649, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.4991, test_acc: 90.0, test_fscore: 90.67, time: 0.47 sec\n",
      "fold: 4, epoch: 34, train_loss: 7.8851, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.5241, test_acc: 90.0, test_fscore: 90.67, time: 0.61 sec\n",
      "fold: 4, epoch: 35, train_loss: 7.5497, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.5149, test_acc: 90.0, test_fscore: 90.67, time: 0.72 sec\n",
      "fold: 4, epoch: 36, train_loss: 8.3671, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.53, test_acc: 90.0, test_fscore: 90.67, time: 0.64 sec\n",
      "fold: 4, epoch: 37, train_loss: 9.4027, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6074, test_acc: 80.0, test_fscore: 81.9, time: 0.53 sec\n",
      "fold: 4, epoch: 38, train_loss: 6.2222, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6613, test_acc: 80.0, test_fscore: 81.9, time: 0.51 sec\n",
      "fold: 4, epoch: 39, train_loss: 8.3858, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7403, test_acc: 80.0, test_fscore: 81.9, time: 0.5 sec\n",
      "fold: 4, epoch: 40, train_loss: 9.4815, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7271, test_acc: 80.0, test_fscore: 81.9, time: 0.44 sec\n",
      "fold: 4, epoch: 41, train_loss: 10.098, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6948, test_acc: 80.0, test_fscore: 81.9, time: 0.47 sec\n",
      "fold: 4, epoch: 42, train_loss: 8.9279, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6794, test_acc: 80.0, test_fscore: 81.9, time: 0.54 sec\n",
      "fold: 4, epoch: 43, train_loss: 8.6986, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7212, test_acc: 80.0, test_fscore: 81.9, time: 0.51 sec\n",
      "fold: 4, epoch: 44, train_loss: 9.6502, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7454, test_acc: 80.0, test_fscore: 81.9, time: 0.46 sec\n",
      "fold: 4, epoch: 45, train_loss: 8.16, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7687, test_acc: 80.0, test_fscore: 81.9, time: 0.48 sec\n",
      "fold: 4, epoch: 46, train_loss: 7.2495, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8193, test_acc: 80.0, test_fscore: 81.9, time: 0.49 sec\n",
      "fold: 4, epoch: 47, train_loss: 9.9021, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8829, test_acc: 80.0, test_fscore: 81.9, time: 0.52 sec\n",
      "fold: 4, epoch: 48, train_loss: 7.2817, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8771, test_acc: 80.0, test_fscore: 81.9, time: 0.51 sec\n",
      "fold: 4, epoch: 49, train_loss: 10.1645, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.9301, test_acc: 80.0, test_fscore: 81.9, time: 0.53 sec\n",
      "fold: 4, epoch: 50, train_loss: 8.9036, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.9543, test_acc: 80.0, test_fscore: 81.9, time: 0.65 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000         8\n",
      "           1     1.0000    1.0000    1.0000         2\n",
      "\n",
      "    accuracy                         1.0000        10\n",
      "   macro avg     1.0000    1.0000    1.0000        10\n",
      "weighted avg     1.0000    1.0000    1.0000        10\n",
      "\n",
      "[[8 0]\n",
      " [0 2]]\n",
      "best model saved in:  /home/zhanghb/program/MyPaperProgram/fairness_personality/code/fusion_spk/speaker_gate_fusion/runs/11-18_19-28_modma_bin_lr5e-05_dropout0.5_bs16_epochs50_fold4/saved_model/modma_model_fold4_fs100.0000.pt\n",
      "Test performance..\n",
      "fold: 4, F-Score: 100.0\n",
      "fold: 4, F-Score-index: 26\n",
      "fold: 4, Acc: 100.0\n",
      "fold: 4, Acc-index: 26\n",
      "Speaker Gate Fusion\n",
      "fold: 5, epoch: 1, train_loss: 10.9768, train_acc: 42.86, train_fscore: 42.86, test_loss: 6.2962, test_acc: 50.0, test_fscore: 33.33, time: 0.67 sec\n",
      "fold: 5, epoch: 2, train_loss: 9.094, train_acc: 61.9, train_fscore: 54.08, test_loss: 6.1154, test_acc: 50.0, test_fscore: 33.33, time: 0.46 sec\n",
      "fold: 5, epoch: 3, train_loss: 8.8876, train_acc: 71.43, train_fscore: 70.16, test_loss: 6.0421, test_acc: 70.0, test_fscore: 67.03, time: 0.58 sec\n",
      "fold: 5, epoch: 4, train_loss: 9.9196, train_acc: 76.19, train_fscore: 75.13, test_loss: 6.044, test_acc: 70.0, test_fscore: 67.03, time: 0.57 sec\n",
      "fold: 5, epoch: 5, train_loss: 8.134, train_acc: 73.81, train_fscore: 72.32, test_loss: 6.0955, test_acc: 60.0, test_fscore: 52.38, time: 0.5 sec\n",
      "fold: 5, epoch: 6, train_loss: 6.8366, train_acc: 76.19, train_fscore: 75.13, test_loss: 6.0884, test_acc: 60.0, test_fscore: 52.38, time: 0.45 sec\n",
      "fold: 5, epoch: 7, train_loss: 8.8299, train_acc: 80.95, train_fscore: 80.49, test_loss: 6.1066, test_acc: 70.0, test_fscore: 67.03, time: 0.36 sec\n",
      "fold: 5, epoch: 8, train_loss: 9.8289, train_acc: 78.57, train_fscore: 77.85, test_loss: 6.1825, test_acc: 60.0, test_fscore: 52.38, time: 0.59 sec\n",
      "fold: 5, epoch: 9, train_loss: 8.4019, train_acc: 80.95, train_fscore: 80.11, test_loss: 6.1974, test_acc: 70.0, test_fscore: 67.03, time: 0.58 sec\n",
      "fold: 5, epoch: 10, train_loss: 8.6348, train_acc: 85.71, train_fscore: 85.37, test_loss: 6.1486, test_acc: 70.0, test_fscore: 67.03, time: 0.63 sec\n",
      "fold: 5, epoch: 11, train_loss: 10.5673, train_acc: 85.71, train_fscore: 85.58, test_loss: 6.0723, test_acc: 70.0, test_fscore: 67.03, time: 0.4 sec\n",
      "fold: 5, epoch: 12, train_loss: 9.8115, train_acc: 88.1, train_fscore: 88.13, test_loss: 6.0684, test_acc: 70.0, test_fscore: 67.03, time: 0.45 sec\n",
      "fold: 5, epoch: 13, train_loss: 9.6213, train_acc: 92.86, train_fscore: 92.89, test_loss: 6.1521, test_acc: 70.0, test_fscore: 67.03, time: 0.57 sec\n",
      "fold: 5, epoch: 14, train_loss: 10.19, train_acc: 95.24, train_fscore: 95.24, test_loss: 6.2364, test_acc: 70.0, test_fscore: 67.03, time: 0.49 sec\n",
      "fold: 5, epoch: 15, train_loss: 6.8051, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.142, test_acc: 70.0, test_fscore: 67.03, time: 0.52 sec\n",
      "fold: 5, epoch: 16, train_loss: 8.2548, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.1079, test_acc: 70.0, test_fscore: 67.03, time: 0.54 sec\n",
      "fold: 5, epoch: 17, train_loss: 9.2294, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.1777, test_acc: 70.0, test_fscore: 67.03, time: 0.39 sec\n",
      "fold: 5, epoch: 18, train_loss: 8.1857, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.4127, test_acc: 70.0, test_fscore: 67.03, time: 0.43 sec\n",
      "fold: 5, epoch: 19, train_loss: 7.7384, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.4271, test_acc: 70.0, test_fscore: 67.03, time: 0.5 sec\n",
      "fold: 5, epoch: 20, train_loss: 7.0582, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.3394, test_acc: 70.0, test_fscore: 67.03, time: 0.51 sec\n",
      "fold: 5, epoch: 21, train_loss: 8.5562, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2569, test_acc: 80.0, test_fscore: 79.17, time: 0.5 sec\n",
      "fold: 5, epoch: 22, train_loss: 7.9186, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.3974, test_acc: 80.0, test_fscore: 79.17, time: 0.47 sec\n",
      "fold: 5, epoch: 23, train_loss: 9.3605, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.4383, test_acc: 80.0, test_fscore: 79.17, time: 0.57 sec\n",
      "fold: 5, epoch: 24, train_loss: 8.4119, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.4401, test_acc: 80.0, test_fscore: 79.17, time: 0.54 sec\n",
      "fold: 5, epoch: 25, train_loss: 7.5506, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.3422, test_acc: 80.0, test_fscore: 79.17, time: 0.42 sec\n",
      "fold: 5, epoch: 26, train_loss: 8.6579, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.3002, test_acc: 80.0, test_fscore: 79.17, time: 0.41 sec\n",
      "fold: 5, epoch: 27, train_loss: 8.2475, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.4069, test_acc: 80.0, test_fscore: 79.17, time: 0.45 sec\n",
      "fold: 5, epoch: 28, train_loss: 9.2113, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.4118, test_acc: 80.0, test_fscore: 79.17, time: 0.52 sec\n",
      "fold: 5, epoch: 29, train_loss: 8.5948, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2995, test_acc: 80.0, test_fscore: 79.17, time: 0.61 sec\n",
      "fold: 5, epoch: 30, train_loss: 9.2676, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.2714, test_acc: 80.0, test_fscore: 79.17, time: 0.43 sec\n",
      "fold: 5, epoch: 31, train_loss: 9.2475, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.1269, test_acc: 80.0, test_fscore: 79.17, time: 0.53 sec\n",
      "fold: 5, epoch: 32, train_loss: 8.0348, train_acc: 100.0, train_fscore: 100.0, test_loss: 6.0144, test_acc: 80.0, test_fscore: 79.17, time: 0.51 sec\n",
      "fold: 5, epoch: 33, train_loss: 8.1962, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.962, test_acc: 80.0, test_fscore: 79.17, time: 0.49 sec\n",
      "fold: 5, epoch: 34, train_loss: 7.702, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.9064, test_acc: 80.0, test_fscore: 79.17, time: 0.49 sec\n",
      "fold: 5, epoch: 35, train_loss: 8.5123, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.9443, test_acc: 80.0, test_fscore: 79.17, time: 0.37 sec\n",
      "fold: 5, epoch: 36, train_loss: 9.7246, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.9299, test_acc: 80.0, test_fscore: 79.17, time: 0.53 sec\n",
      "fold: 5, epoch: 37, train_loss: 8.105, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8895, test_acc: 80.0, test_fscore: 79.17, time: 0.5 sec\n",
      "fold: 5, epoch: 38, train_loss: 7.5344, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8584, test_acc: 90.0, test_fscore: 89.9, time: 0.53 sec\n",
      "fold: 5, epoch: 39, train_loss: 8.0518, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8517, test_acc: 90.0, test_fscore: 89.9, time: 0.47 sec\n",
      "fold: 5, epoch: 40, train_loss: 7.5039, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8709, test_acc: 90.0, test_fscore: 89.9, time: 0.49 sec\n",
      "fold: 5, epoch: 41, train_loss: 9.4146, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.9297, test_acc: 80.0, test_fscore: 79.17, time: 0.46 sec\n",
      "fold: 5, epoch: 42, train_loss: 7.7134, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8713, test_acc: 90.0, test_fscore: 89.9, time: 0.51 sec\n",
      "fold: 5, epoch: 43, train_loss: 7.9062, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8357, test_acc: 90.0, test_fscore: 89.9, time: 0.73 sec\n",
      "fold: 5, epoch: 44, train_loss: 8.7287, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.9068, test_acc: 90.0, test_fscore: 89.9, time: 0.7 sec\n",
      "fold: 5, epoch: 45, train_loss: 8.9762, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.8653, test_acc: 90.0, test_fscore: 89.9, time: 0.62 sec\n",
      "fold: 5, epoch: 46, train_loss: 7.907, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7745, test_acc: 90.0, test_fscore: 89.9, time: 0.52 sec\n",
      "fold: 5, epoch: 47, train_loss: 10.0021, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.6847, test_acc: 90.0, test_fscore: 89.9, time: 0.43 sec\n",
      "fold: 5, epoch: 48, train_loss: 9.064, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7569, test_acc: 90.0, test_fscore: 89.9, time: 0.64 sec\n",
      "fold: 5, epoch: 49, train_loss: 8.3086, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7826, test_acc: 90.0, test_fscore: 89.9, time: 0.61 sec\n",
      "fold: 5, epoch: 50, train_loss: 8.0742, train_acc: 100.0, train_fscore: 100.0, test_loss: 5.7862, test_acc: 90.0, test_fscore: 89.9, time: 0.82 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    1.0000    0.9091         5\n",
      "           1     1.0000    0.8000    0.8889         5\n",
      "\n",
      "    accuracy                         0.9000        10\n",
      "   macro avg     0.9167    0.9000    0.8990        10\n",
      "weighted avg     0.9167    0.9000    0.8990        10\n",
      "\n",
      "[[5 0]\n",
      " [1 4]]\n",
      "best model saved in:  /home/zhanghb/program/MyPaperProgram/fairness_personality/code/fusion_spk/speaker_gate_fusion/runs/11-18_19-28_modma_bin_lr5e-05_dropout0.5_bs16_epochs50_fold5/saved_model/modma_model_fold5_fs89.9000.pt\n",
      "Test performance..\n",
      "fold: 5, F-Score: 89.9\n",
      "fold: 5, F-Score-index: 38\n",
      "fold: 5, Acc: 90.0\n",
      "fold: 5, Acc-index: 38\n",
      "5-fold ave fscore: 96.19200000000001\n",
      "5-fold ave acc: 96.18199999999999\n"
     ]
    }
   ],
   "source": [
    "test_label_list_perfold, test_pred_list_perfold, test_id_list_perfold,best_test_acc, best_test_fs = run(dataset=dataset,\n",
    "                                                                                                        dataset_name=dataset_name,\n",
    "                                                                                                        logdir_root=logdir_root,\n",
    "                                                                                                        classify_flag=classify_flag,\n",
    "                                                                                                        device=device,\n",
    "                                                                                                        D_audio=D_audio,\n",
    "                                                                                                        D_text=768,\n",
    "                                                                                                        gamma=gamma,\n",
    "                                                                                                        SEED=SEED,\n",
    "                                                                                                        BS=BS,\n",
    "                                                                                                        LR=LR,\n",
    "                                                                                                        Weight_Decay=Weight_Decay,\n",
    "                                                                                                        Dropout=Dropout,\n",
    "                                                                                                        EPOCHS=EPOCHS,\n",
    "                                                                                                        K=5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2. Node Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "zhanghb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
